{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_x_df = pd.read_csv('X.csv')\n",
    "data_t_df = pd.read_csv('T.csv')\n",
    "data_df = pd.concat([data_x_df, data_t_df], axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(data_df)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "\n",
    "train_feature = dataset[:train_size, :-1]\n",
    "test_feature = dataset[train_size:, :-1]\n",
    "train_target = dataset[:train_size, -1]\n",
    "test_target = dataset[train_size:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_weight_ml(phi, target):\n",
    "    return np.linalg.pinv(phi) @ target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M = 1\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = \\textit{w}_{0} + \\sum^{D}_{\\textit{i} = 1} \\textit{w}_{i}\\textit{x}_{i} = \\textbf{w}^{T} \\phi(x) $$\n",
    "where\n",
    "$$ \\textbf{w} = \\begin{pmatrix} \\textit{w}_{0} & \\textit{w}_{1} & ... & \\textit{w}_{D} \\end{pmatrix}^{T} $$\n",
    "$$ \\phi(x) = \\begin{pmatrix} 1 & \\textit{x}_{1} & ... & \\textit{x}_{D} \\end{pmatrix}^{T} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m1(features):\n",
    "    \"\"\"Computes phi matrix with m = 1.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "    return np.concatenate((m_0, m_1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_m1 = phi_m1(train_feature)\n",
    "test_phi_m1 = phi_m1(test_feature)\n",
    "\n",
    "w_m1 = optimal_weight_ml(train_phi_m1, train_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M = 2\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = \\textit{w}_{0} + \\sum^{D}_{\\textit{i} = 1} \\textit{w}_{i}\\textit{x}_{i} + \\sum^{D}_{\\textit{i} = 1}\\sum^{D}_{\\textit{j} = 1} \\textit{w}_{ij}\\textit{x}_{i}\\textit{x}_{j} = \\textbf{w}^{T} \\phi(x) $$\n",
    "where\n",
    "$$ \\textbf{w} = \\begin{pmatrix} \\textit{w}_{0} & \\textit{w}_{1} & ... & \\textit{w}_{D} & \\textit{w}_{11} & \\textit{w}_{12} & ... & \\textit{w}_{DD} \\end{pmatrix}^{T} $$\n",
    "$$ \\phi(x) = \\begin{pmatrix} 1 & \\textit{x}_{1} & ... & \\textit{x}_{D} & \\textit{x}_{1}\\textit{x}_{1} & \\textit{x}_{1}\\textit{x}_{2} & ... & \\textit{x}_{D}\\textit{x}_{D} \\end{pmatrix}^{T} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m2(features):\n",
    "    \"\"\"Computes phi matrix with m = 2.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features + n_features ** 2)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "\n",
    "    # x_1^2, x_1x_2, ..., x_1x_D, x_2^2, x_2x_3, ..., x_2x_D, ..., x_D^2\n",
    "    m_2 = np.expand_dims(features, axis=-1) @ np.expand_dims(features, axis=-2)\n",
    "    m_2 = m_2.reshape(m_2.shape[0], -1)\n",
    "    return np.concatenate((m_0, m_1, m_2), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_m2 = phi_m2(train_feature)\n",
    "test_phi_m2 = phi_m2(test_feature)\n",
    "\n",
    "w_m2 = optimal_weight_ml(train_phi_m2, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_error(y, t):\n",
    "    return np.sqrt(np.mean((y - t)**2))\n",
    "\n",
    "\n",
    "train_rms_m1 = root_mean_square_error(train_phi_m1 @ w_m1, train_target)\n",
    "test_rms_m1 = root_mean_square_error(test_phi_m1 @ w_m1, test_target)\n",
    "\n",
    "train_rms_m2 = root_mean_square_error(train_phi_m2 @ w_m2, train_target)\n",
    "test_rms_m2 = root_mean_square_error(test_phi_m2 @ w_m2, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train rms m1: {train_rms_m1}')\n",
    "print(f'test rms m1: {test_rms_m1}')\n",
    "print(f'train rms m2: {train_rms_m2}')\n",
    "print(f'test rms m2: {test_rms_m2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Analysis of M = 1\n",
    "\n",
    "I used the weight of the model to analyze the importance of each feature. The weight of the model is the coefficient of each feature. The larger the weight is, the more important the feature is.\n",
    "\n",
    "> The weight with index 0 is not considered as a feature because it is the bias of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w_m1: {w_m1}')\n",
    "abs_importance = np.abs(w_m1[1:]) # index #0 is bias\n",
    "importance_rank = np.argsort(abs_importance)[::-1]\n",
    "print(f'Rank of features\\' contribution: {importance_rank}')\n",
    "print(f'The most important feature: ${importance_rank[0]} {data_df.columns[importance_rank[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    weight_without_i_feature = np.copy(w_m1)\n",
    "    weight_without_i_feature[i + 1] = 0 # index #0 is bias\n",
    "\n",
    "    train_rms_without_i_feature = root_mean_square_error(train_phi_m1 @ weight_without_i_feature, train_target)\n",
    "    test_rms_without_i_feature = root_mean_square_error(test_phi_m1 @ weight_without_i_feature, test_target)\n",
    "\n",
    "    print(f'Remove feature #{i}:')\n",
    "    print(f'train rms m1: {train_rms_without_i_feature}, test rms m1: {test_rms_without_i_feature}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without feature #7 (density), the rms error is larger than the model without other features, which means that feature #7 is the most important feature in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(features):\n",
    "    \"\"\"Computes the mean and standard deviation of each feature.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "    \"\"\"\n",
    "    mean = np.mean(features, axis=0, keepdims=True)\n",
    "    std = np.std(features, axis=0, keepdims=True)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def phi_gaussian(features, mean, std):\n",
    "    \"\"\"Computes the Gaussian phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    gaussians = np.exp(-((features - mean) / std)**2 / 2)\n",
    "    return np.concatenate((np.ones((features.shape[0], 1)), gaussians), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = get_mean_and_std(train_feature)\n",
    "train_phi_gaussian = phi_gaussian(train_feature, train_mean, train_std)\n",
    "test_phi_gaussian = phi_gaussian(test_feature, train_mean, train_std)\n",
    "\n",
    "w_gaussian = optimal_weight_ml(train_phi_gaussian, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_gaussian = root_mean_square_error(train_phi_gaussian @ w_gaussian, train_target)\n",
    "test_rms_gaussian = root_mean_square_error(test_phi_gaussian @ w_gaussian, test_target)\n",
    "\n",
    "print(f'train rms gaussian: {train_rms_gaussian}')\n",
    "print(f'test rms gaussian: {test_rms_gaussian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_sigmoid(features, mean, std):\n",
    "    \"\"\"Computes the sigmoid phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    sigmoids = 1 / (1 + np.exp(-(features - mean) / std))\n",
    "    return np.concatenate((np.ones((features.shape[0], 1)), sigmoids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_sigmoid = phi_sigmoid(train_feature, train_mean, train_std)\n",
    "test_phi_sigmoid = phi_sigmoid(test_feature, train_mean, train_std)\n",
    "\n",
    "w_sigmoid = optimal_weight_ml(train_phi_sigmoid, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_sigmoid = root_mean_square_error(train_phi_sigmoid @ w_sigmoid, train_target)\n",
    "test_rms_sigmoid = root_mean_square_error(test_phi_sigmoid @ w_sigmoid, test_target)\n",
    "\n",
    "print(f'train rms sigmoid: {train_rms_sigmoid}')\n",
    "print(f'test rms sigmoid: {test_rms_sigmoid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_hybrid(features, mean, std):\n",
    "    \"\"\"Computes the hybrid phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "    # x_1^2, x_1x_2, ..., x_1x_D, x_2^2, x_2x_3, ..., x_2x_D, ..., x_D^2\n",
    "    m_2 = np.expand_dims(features, axis=-1) @ np.expand_dims(features, axis=-2)\n",
    "    m_2 = m_2.reshape(m_2.shape[0], -1)\n",
    "    gaussians = np.exp(-((features - mean) / std)**2 / 2)\n",
    "    sigmoids = 1 / (1 + np.exp(-(features - mean) / std))\n",
    "    return np.concatenate((m_0, m_1, m_2, gaussians, sigmoids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_hybrid = phi_hybrid(train_feature, train_mean, train_std)\n",
    "test_phi_hybrid = phi_hybrid(test_feature, train_mean, train_std)\n",
    "\n",
    "w_hybrid = optimal_weight_ml(train_phi_hybrid, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_hybrid = root_mean_square_error(train_phi_hybrid @ w_hybrid, train_target)\n",
    "test_rms_hybrid = root_mean_square_error(test_phi_hybrid @ w_hybrid, test_target)\n",
    "\n",
    "print(f'train rms hybrid: {train_rms_hybrid}')\n",
    "print(f'test rms hybrid: {test_rms_hybrid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Fold Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold(data, n_split):\n",
    "    \"\"\"Splits the data into n_fold.\n",
    "    \n",
    "    Args:\n",
    "        data: np.ndarray with shape (n_samples, n_features + 1)\n",
    "        n_split: int\n",
    "        \n",
    "    Yields:\n",
    "        train_index: np.ndarray with shape (n_samples * (n_split - 1) // n_split, )\n",
    "        test_index: np.ndarray with shape (n_samples // n_split, )\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_samples_per_fold = n_samples // n_split\n",
    "    for i in range(n_split):\n",
    "        train_index = np.concatenate((\n",
    "            np.arange(0, i * n_samples_per_fold),\n",
    "            np.arange((i + 1) * n_samples_per_fold, n_samples),\n",
    "        ))\n",
    "        test_index = np.arange(\n",
    "            i * n_samples_per_fold,\n",
    "            (i + 1) * n_samples_per_fold,\n",
    "        )\n",
    "        yield train_index, test_index\n",
    "\n",
    "\n",
    "def fit_ml(train_feature, train_target, test_feature, test_taarget, mean, std, phi_fn):\n",
    "    \"\"\"Fits the phi function.\n",
    "    \n",
    "    Args:\n",
    "        train_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        train_target: np.ndarray with shape (n_samples, 1)\n",
    "        test_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        test_target: np.ndarray with shape (n_samples, 1)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        phi_fn: function with signature (features, mean, std) -> phi\n",
    "        \n",
    "    Returns:\n",
    "        train_rms: float\n",
    "        test_rms: float\n",
    "        weights: np.ndarray with shape (1 + n_features, 1)\n",
    "    \"\"\"\n",
    "    train_phi = phi_fn(train_feature, mean, std)\n",
    "    test_phi = phi_fn(test_feature, mean, std)\n",
    "    weights = optimal_weight_ml(train_phi, train_target)\n",
    "    train_rms = root_mean_square_error(train_phi @ weights, train_target)\n",
    "    test_rms = root_mean_square_error(test_phi @ weights, test_target)\n",
    "    return train_rms, test_rms, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_gaussian_list = []\n",
    "test_rms_gaussian_list = []\n",
    "train_rms_sigmoid_list = []\n",
    "test_rms_sigmoid_list = []\n",
    "train_rms_hybrid_list = []\n",
    "test_rms_hybrid_list = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(n_fold(dataset, 5)):\n",
    "    train_feature, train_target = dataset[train_index, :-1], dataset[train_index, -1]\n",
    "    test_feature, test_target = dataset[test_index, :-1], dataset[test_index, -1]\n",
    "\n",
    "    train_mean, train_std = get_mean_and_std(train_feature)\n",
    "\n",
    "    # gaussian\n",
    "    train_rms_gaussian, test_rms_gaussian, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                      test_target, train_mean, train_std,\n",
    "                                                      phi_gaussian)\n",
    "\n",
    "    train_rms_gaussian_list.append(train_rms_gaussian)\n",
    "    test_rms_gaussian_list.append(test_rms_gaussian)\n",
    "\n",
    "    # sigmoid\n",
    "    train_rms_sigmoid, test_rms_sigmoid, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                    test_target, train_mean, train_std, phi_sigmoid)\n",
    "\n",
    "    train_rms_sigmoid_list.append(train_rms_sigmoid)\n",
    "    test_rms_sigmoid_list.append(test_rms_sigmoid)\n",
    "\n",
    "    # hybrid\n",
    "    train_rms_hybrid, test_rms_hybrid, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                  test_target, train_mean, train_std, phi_hybrid)\n",
    "\n",
    "    train_rms_hybrid_list.append(train_rms_hybrid)\n",
    "    test_rms_hybrid_list.append(test_rms_hybrid)\n",
    "\n",
    "print(f'average train rms gaussian: {np.mean(train_rms_gaussian_list)}')\n",
    "print(f'average test rms gaussian: {np.mean(test_rms_gaussian_list)}')\n",
    "print(f'average train rms sigmoid: {np.mean(train_rms_sigmoid_list)}')\n",
    "print(f'average test rms sigmoid: {np.mean(test_rms_sigmoid_list)}')\n",
    "print(f'average train rms hybrid: {np.mean(train_rms_hybrid_list)}')\n",
    "print(f'average test rms hybrid: {np.mean(test_rms_hybrid_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum A Posteriori Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Approach\n",
    "\n",
    "Maximize the likelihood of the data given the model, where the likelihood $ p(t | \\textbf{x}, \\textbf{w}, \\beta) = \\mathcal{N}(t | y(\\textbf{x}, \\textbf{w}),\\beta^{-1}) $.\n",
    "\n",
    "This approach is equivalent to minimizing the mean square error $ \\textit{E}_{D}(\\textbf{w}) = \\frac{1}{2} \\sum^{N}_{n=1}{\\{t_{n} - \\textbf{w}^{T} \\phi(\\textbf{x}_{n}) \\}} $. The optimal solution is given by the normal equation $ \\textbf{w}_{ML} = (\\Phi^{T} \\Phi)^{-1} \\Phi^{T} \\textbf{t} $.\n",
    "\n",
    "### Maximum A Posteriori Approach\n",
    "\n",
    "Assume that the prior distribution of the weight is a Gaussian distribution with mean $ 0 $ and variance $ \\alpha^{-1} $, which is $ p(\\textbf{w} | \\alpha) = \\mathcal{N}(\\textbf{w} | \\textbf{0}, \\alpha^{-1}\\textbf{I}) $.\n",
    "\n",
    "Maximize the posterior probability of the model given the data, where the posterior probability $ p(\\textbf{w} | \\textbf{t}, \\textbf{x}, \\alpha, \\beta) = \\mathcal{N}(\\textbf{w} | \\beta(\\alpha\\textbf{I} + \\beta\\Phi^{T}\\Phi)^{-1}\\Phi^{T}\\textbf{t}, (\\alpha\\textbf{I} + \\beta\\Phi^{T}\\Phi)^{-1}) $.\n",
    "\n",
    "This approach is equivalent to minimizing the mean square error $ \\textit{E}_{D}(\\textbf{w}) = \\frac{1}{2} \\sum^{N}_{n=1}{\\{t_{n} - \\textbf{w}^{T} \\phi(\\textbf{x}_{n}) \\}} + \\frac{\\lambda}{2} \\textbf{w}^{T} \\textbf{w} $, where $ \\lambda = \\frac{\\alpha}{\\beta} $. The optimal solution is given by the normal equation $ \\textbf{w}_{MAP} = (\\lambda\\textbf{I} + \\Phi^{T} \\Phi)^{-1} \\Phi^{T} \\textbf{t} $.\n",
    "\n",
    "With assuming the distribution of the weight, the model is more robust to the noise of the data. The model with the maximum a posteriori approach is more stable than the model with the maximum likelihood approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_weight_map(phi, target, lamb):\n",
    "    return np.linalg.inv((np.eye(phi.shape[1]) * lamb + phi.T @ phi)) @ phi.T @ target\n",
    "\n",
    "\n",
    "def fit_map(train_feature, train_target, test_feature, test_target, mean, std, phi_fn, lamb):\n",
    "    \"\"\"Fits the phi function.\n",
    "    \n",
    "    Args:\n",
    "        train_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        train_target: np.ndarray with shape (n_samples, 1)\n",
    "        test_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        test_target: np.ndarray with shape (n_samples, 1)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        phi_fn: function with signature (features, mean, std) -> phi\n",
    "        lamb: float\n",
    "        \n",
    "    Returns:\n",
    "        train_rms: float\n",
    "        test_rms: float\n",
    "        weights: np.ndarray with shape (1 + n_features, 1)\n",
    "    \"\"\"\n",
    "    train_phi = phi_fn(train_feature, mean, std)\n",
    "    test_phi = phi_fn(test_feature, mean, std)\n",
    "    weights = optimal_weight_map(train_phi, train_target, lamb)\n",
    "    train_rms = root_mean_square_error(train_phi @ weights, train_target)\n",
    "    test_rms = root_mean_square_error(test_phi @ weights, test_target)\n",
    "    return train_rms, test_rms, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 3\n",
    "\n",
    "train_rms_gaussian_list = []\n",
    "test_rms_gaussian_list = []\n",
    "train_rms_sigmoid_list = []\n",
    "test_rms_sigmoid_list = []\n",
    "train_rms_hybrid_list = []\n",
    "test_rms_hybrid_list = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(n_fold(dataset, 5)):\n",
    "    train_feature, train_target = dataset[train_index, :-1], dataset[train_index, -1]\n",
    "    test_feature, test_target = dataset[test_index, :-1], dataset[test_index, -1]\n",
    "\n",
    "    train_mean, train_std = get_mean_and_std(train_feature)\n",
    "\n",
    "    # gaussian\n",
    "    train_rms_gaussian, test_rms_gaussian, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                       test_target, train_mean, train_std,\n",
    "                                                       phi_gaussian, LAMBDA)\n",
    "\n",
    "    train_rms_gaussian_list.append(train_rms_gaussian)\n",
    "    test_rms_gaussian_list.append(test_rms_gaussian)\n",
    "\n",
    "    # sigmoid\n",
    "    train_rms_sigmoid, test_rms_sigmoid, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                     test_target, train_mean, train_std,\n",
    "                                                     phi_sigmoid, LAMBDA)\n",
    "\n",
    "    train_rms_sigmoid_list.append(train_rms_sigmoid)\n",
    "    test_rms_sigmoid_list.append(test_rms_sigmoid)\n",
    "\n",
    "    # hybrid\n",
    "    train_rms_hybrid, test_rms_hybrid, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                   test_target, train_mean, train_std, phi_hybrid,\n",
    "                                                   LAMBDA)\n",
    "\n",
    "    train_rms_hybrid_list.append(train_rms_hybrid)\n",
    "    test_rms_hybrid_list.append(test_rms_hybrid)\n",
    "\n",
    "print(f'average train rms gaussian: {np.mean(train_rms_gaussian_list)}')\n",
    "print(f'average test rms gaussian: {np.mean(test_rms_gaussian_list)}')\n",
    "print(f'average train rms sigmoid: {np.mean(train_rms_sigmoid_list)}')\n",
    "print(f'average test rms sigmoid: {np.mean(test_rms_sigmoid_list)}')\n",
    "print(f'average train rms hybrid: {np.mean(train_rms_hybrid_list)}')\n",
    "print(f'average test rms hybrid: {np.mean(test_rms_hybrid_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('hw1-nKNpIY9G-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9347332e7fffacb469f6b61eb89afbd5d1cb4d5d929849d41d99be57377ebf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
