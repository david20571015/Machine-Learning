{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_x_df = pd.read_csv('X.csv')\n",
    "data_t_df = pd.read_csv('T.csv')\n",
    "data_df = pd.concat([data_x_df, data_t_df], axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(data_df)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "\n",
    "train_feature = dataset[:train_size, :-1]\n",
    "test_feature = dataset[train_size:, :-1]\n",
    "train_target = dataset[:train_size, -1]\n",
    "test_target = dataset[train_size:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_weight_ml(phi, target):\n",
    "    return np.linalg.pinv(phi) @ target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M = 1\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = \\textit{w}_{0} + \\sum^{D}_{\\textit{i} = 1} \\textit{w}_{i}\\textit{x}_{i} = \\textbf{w}^{T} \\phi(x) $$\n",
    "where\n",
    "$$ \\textbf{w} = \\begin{pmatrix} \\textit{w}_{0} & \\textit{w}_{1} & ... & \\textit{w}_{D} \\end{pmatrix}^{T} $$\n",
    "$$ \\phi(x) = \\begin{pmatrix} 1 & \\textit{x}_{1} & ... & \\textit{x}_{D} \\end{pmatrix}^{T} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m1(features):\n",
    "    \"\"\"Computes phi matrix with m = 1.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "    return np.concatenate((m_0, m_1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_m1 = phi_m1(train_feature)\n",
    "test_phi_m1 = phi_m1(test_feature)\n",
    "\n",
    "w_m1 = optimal_weight_ml(train_phi_m1, train_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M = 2\n",
    "\n",
    "$$ y(\\textbf{x}, \\textbf{w}) = \\textit{w}_{0} + \\sum^{D}_{\\textit{i} = 1} \\textit{w}_{i}\\textit{x}_{i} + \\sum^{D}_{\\textit{i} = 1}\\sum^{D}_{\\textit{j} = 1} \\textit{w}_{ij}\\textit{x}_{i}\\textit{x}_{j} = \\textbf{w}^{T} \\phi(x) $$\n",
    "where\n",
    "$$ \\textbf{w} = \\begin{pmatrix} \\textit{w}_{0} & \\textit{w}_{1} & ... & \\textit{w}_{D} & \\textit{w}_{11} & \\textit{w}_{12} & ... & \\textit{w}_{DD} \\end{pmatrix}^{T} $$\n",
    "$$ \\phi(x) = \\begin{pmatrix} 1 & \\textit{x}_{1} & ... & \\textit{x}_{D} & \\textit{x}_{1}\\textit{x}_{1} & \\textit{x}_{1}\\textit{x}_{2} & ... & \\textit{x}_{D} \\textit{x}_{D} \\end{pmatrix}^{T} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m2(features):\n",
    "    \"\"\"Computes phi matrix with m = 2.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features + n_features ** 2)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "\n",
    "    # x_1^2, x_1x_2, ..., x_1x_D, x_2^2, x_2x_3, ..., x_2x_D, ..., x_D^2\n",
    "    m_2 = np.expand_dims(features, axis=-1) @ np.expand_dims(features, axis=-2)\n",
    "    m_2 = m_2.reshape(m_2.shape[0], -1)\n",
    "    return np.concatenate((m_0, m_1, m_2), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_m2 = phi_m2(train_feature)\n",
    "test_phi_m2 = phi_m2(test_feature)\n",
    "\n",
    "w_m2 = optimal_weight_ml(train_phi_m2, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_error(y, t):\n",
    "    return np.sqrt(np.mean((y - t)**2))\n",
    "\n",
    "\n",
    "train_rms_m1 = root_mean_square_error(train_phi_m1 @ w_m1, train_target)\n",
    "test_rms_m1 = root_mean_square_error(test_phi_m1 @ w_m1, test_target)\n",
    "\n",
    "train_rms_m2 = root_mean_square_error(train_phi_m2 @ w_m2, train_target)\n",
    "test_rms_m2 = root_mean_square_error(test_phi_m2 @ w_m2, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train rms m1: {train_rms_m1}')\n",
    "print(f'test rms m1: {test_rms_m1}')\n",
    "print(f'train rms m2: {train_rms_m2}')\n",
    "print(f'test rms m2: {test_rms_m2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Analysis of M = 1\n",
    "\n",
    "I used the weight of the model to analyze the importance of each feature. The weight of the model is the coefficient of each feature. The larger the weight is, the more important the feature is.\n",
    "\n",
    "> The weight with index 0 is not considered as a feature because it is the bias of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w_m1: {w_m1}')\n",
    "abs_importance = np.abs(w_m1[1:])  # index #0 is bias\n",
    "importance_rank = np.argsort(abs_importance)[::-1]\n",
    "print(f'Rank of features\\' contribution: {importance_rank}')\n",
    "print(f'The most important feature: #{importance_rank[0]} {data_df.columns[importance_rank[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    weight_without_i_feature = np.copy(w_m1)\n",
    "    weight_without_i_feature[i + 1] = 0  # index #0 is bias\n",
    "\n",
    "    train_rms_without_i_feature = root_mean_square_error(train_phi_m1 @ weight_without_i_feature,\n",
    "                                                         train_target)\n",
    "    test_rms_without_i_feature = root_mean_square_error(test_phi_m1 @ weight_without_i_feature,\n",
    "                                                        test_target)\n",
    "\n",
    "    print(f'Remove feature #{i}:')\n",
    "    print(f'train rms m1: {train_rms_without_i_feature}, test rms m1: {test_rms_without_i_feature}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without feature #7 (density), the rms error is larger than the model without other features, which means that feature #7 is the most important feature in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gaussian basis, sigmoid basis and hybrid basis which is the combination of first and second order polynomial, Gaussian and sigmoid basis.\n",
    "\n",
    "- Gaussian basis: The output is larger when the input is closer to the mean of the basis. It is suitable for get rid of extreme values.\n",
    "- Sigmoid basis: The output remains the same order of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(features):\n",
    "    \"\"\"Computes the mean and standard deviation of each feature.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "    \"\"\"\n",
    "    mean = np.mean(features, axis=0, keepdims=True)\n",
    "    std = np.std(features, axis=0, keepdims=True)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def phi_gaussian(features, mean, std):\n",
    "    \"\"\"Computes the Gaussian phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    gaussians = np.exp(-((features - mean) / std)**2 / 2)\n",
    "    return np.concatenate((m_0, gaussians), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = get_mean_and_std(train_feature)\n",
    "train_phi_gaussian = phi_gaussian(train_feature, train_mean, train_std)\n",
    "test_phi_gaussian = phi_gaussian(test_feature, train_mean, train_std)\n",
    "\n",
    "w_gaussian = optimal_weight_ml(train_phi_gaussian, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_gaussian = root_mean_square_error(train_phi_gaussian @ w_gaussian, train_target)\n",
    "test_rms_gaussian = root_mean_square_error(test_phi_gaussian @ w_gaussian, test_target)\n",
    "\n",
    "print(f'train rms gaussian: {train_rms_gaussian}')\n",
    "print(f'test rms gaussian: {test_rms_gaussian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_sigmoid(features, mean, std):\n",
    "    \"\"\"Computes the sigmoid phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    sigmoids = 1 / (1 + np.exp(-(features - mean) / std))\n",
    "    return np.concatenate((m_0, sigmoids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_sigmoid = phi_sigmoid(train_feature, train_mean, train_std)\n",
    "test_phi_sigmoid = phi_sigmoid(test_feature, train_mean, train_std)\n",
    "\n",
    "w_sigmoid = optimal_weight_ml(train_phi_sigmoid, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_sigmoid = root_mean_square_error(train_phi_sigmoid @ w_sigmoid, train_target)\n",
    "test_rms_sigmoid = root_mean_square_error(test_phi_sigmoid @ w_sigmoid, test_target)\n",
    "\n",
    "print(f'train rms sigmoid: {train_rms_sigmoid}')\n",
    "print(f'test rms sigmoid: {test_rms_sigmoid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_hybrid(features, mean, std):\n",
    "    \"\"\"Computes the hybrid phi.\n",
    "    \n",
    "    Args:\n",
    "        features: np.ndarray with shape (n_samples, n_features)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        phi: np.ndarray with shape (n_samples, 1 + n_features + n_features ** 2 + n_features + n_features)\n",
    "    \"\"\"\n",
    "    m_0 = np.ones((features.shape[0], 1))  # 1\n",
    "    m_1 = features  # x_1, x_2, ..., x_D\n",
    "    # x_1^2, x_1x_2, ..., x_1x_D, x_2^2, x_2x_3, ..., x_2x_D, ..., x_D^2\n",
    "    m_2 = np.expand_dims(features, axis=-1) @ np.expand_dims(features, axis=-2)\n",
    "    m_2 = m_2.reshape(m_2.shape[0], -1)\n",
    "    gaussians = np.exp(-((features - mean) / std)**2 / 2)\n",
    "    sigmoids = 1 / (1 + np.exp(-(features - mean) / std))\n",
    "    return np.concatenate((m_0, m_1, m_2, gaussians, sigmoids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phi_hybrid = phi_hybrid(train_feature, train_mean, train_std)\n",
    "test_phi_hybrid = phi_hybrid(test_feature, train_mean, train_std)\n",
    "\n",
    "w_hybrid = optimal_weight_ml(train_phi_hybrid, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_hybrid = root_mean_square_error(train_phi_hybrid @ w_hybrid, train_target)\n",
    "test_rms_hybrid = root_mean_square_error(test_phi_hybrid @ w_hybrid, test_target)\n",
    "\n",
    "print(f'train rms hybrid: {train_rms_hybrid}')\n",
    "print(f'test rms hybrid: {test_rms_hybrid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum likelihood:')\n",
    "\n",
    "pd.DataFrame(\n",
    "    data={\n",
    "        'train rms': [\n",
    "            np.mean(train_rms_m1),\n",
    "            np.mean(train_rms_m2),\n",
    "            np.mean(train_rms_gaussian),\n",
    "            np.mean(train_rms_sigmoid),\n",
    "            np.mean(train_rms_hybrid),\n",
    "        ],\n",
    "        'test rms': [\n",
    "            np.mean(test_rms_m1),\n",
    "            np.mean(test_rms_m2),\n",
    "            np.mean(test_rms_gaussian),\n",
    "            np.mean(test_rms_sigmoid),\n",
    "            np.mean(test_rms_hybrid),\n",
    "        ]\n",
    "    },\n",
    "    index=[\n",
    "        'polynomial m=1',\n",
    "        'polynomial m=2',\n",
    "        'gaussian',\n",
    "        'sigmoid',\n",
    "        'hybrid',\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation, Gaussian basis is the worst in the testing dataset. Other basis have almost similar results. All of the basis except Gaussian basis remain the order of the inputs and outputs. Therefore, we can infer that most features are highly correlated.\n",
    "\n",
    "Hybrid basis performs the best in the training dataset, but not in the testing dataset, which means that the model is too complex in this case. The model is overfitting, and it is not general enough to predict the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Fold Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_fold(data, n_split):\n",
    "    \"\"\"Splits the data into n_fold.\n",
    "    \n",
    "    Args:\n",
    "        data: np.ndarray with shape (n_samples, ...)\n",
    "        n_split: int\n",
    "        \n",
    "    Yields:\n",
    "        train_index: np.ndarray with shape (n_samples * (n_split - 1) // n_split, ...)\n",
    "        test_index: np.ndarray with shape (n_samples // n_split, ...)\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    n_samples_per_fold = n_samples // n_split\n",
    "    for i in range(n_split):\n",
    "        train_index = np.concatenate((\n",
    "            np.arange(0, i * n_samples_per_fold),\n",
    "            np.arange((i + 1) * n_samples_per_fold, n_samples),\n",
    "        ))\n",
    "        test_index = np.arange(\n",
    "            i * n_samples_per_fold,\n",
    "            (i + 1) * n_samples_per_fold,\n",
    "        )\n",
    "        yield train_index, test_index\n",
    "\n",
    "\n",
    "def fit_ml(train_feature, train_target, test_feature, test_target, mean, std, phi_fn):\n",
    "    \"\"\"Fits the phi function.\n",
    "    \n",
    "    Args:\n",
    "        train_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        train_target: np.ndarray with shape (n_samples, 1)\n",
    "        test_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        test_target: np.ndarray with shape (n_samples, 1)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        phi_fn: function with signature (features, mean, std) -> phi\n",
    "        \n",
    "    Returns:\n",
    "        train_rms: float\n",
    "        test_rms: float\n",
    "        weights: np.ndarray with shape (phi.shape[1], 1)\n",
    "    \"\"\"\n",
    "    train_phi = phi_fn(train_feature, mean, std)\n",
    "    test_phi = phi_fn(test_feature, mean, std)\n",
    "    weights = optimal_weight_ml(train_phi, train_target)\n",
    "    train_rms = root_mean_square_error(train_phi @ weights, train_target)\n",
    "    test_rms = root_mean_square_error(test_phi @ weights, test_target)\n",
    "    return train_rms, test_rms, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 5-fold to select the best sigmoid model from different std, which are [50%, 75%, 100%, 125%, 150%] of the original std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rms_std_50_list, test_rms_std_50_list = [], []\n",
    "train_rms_std_75_list, test_rms_std_75_list = [], []\n",
    "train_rms_std_100_list, test_rms_std_100_list = [], []\n",
    "train_rms_std_125_list, test_rms_std_125_list = [], []\n",
    "train_rms_std_150_list, test_rms_std_150_list = [], []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(n_fold(dataset, 5)):\n",
    "    train_feature, train_target = dataset[train_index, :-1], dataset[train_index, -1]\n",
    "    test_feature, test_target = dataset[test_index, :-1], dataset[test_index, -1]\n",
    "\n",
    "    train_mean, train_std = get_mean_and_std(train_feature)\n",
    "\n",
    "    # 50%\n",
    "    train_rms_std_50, test_rms_std_50, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                  test_target, train_mean, train_std * 0.5,\n",
    "                                                  phi_sigmoid)\n",
    "\n",
    "    train_rms_std_50_list.append(train_rms_std_50)\n",
    "    test_rms_std_50_list.append(test_rms_std_50)\n",
    "\n",
    "    # 75%\n",
    "    train_rms_std_75, test_rms_std_75, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                    test_target, train_mean, train_std * 0.75,\n",
    "                                                    phi_sigmoid)\n",
    "\n",
    "    train_rms_std_75_list.append(train_rms_std_75)\n",
    "    test_rms_std_75_list.append(test_rms_std_75)\n",
    "\n",
    "    # 100%\n",
    "    train_rms_std_100, test_rms_std_100, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                    test_target, train_mean, train_std,\n",
    "                                                    phi_sigmoid)\n",
    "\n",
    "    train_rms_std_100_list.append(train_rms_std_100)\n",
    "    test_rms_std_100_list.append(test_rms_std_100)\n",
    "\n",
    "    # 125%\n",
    "    train_rms_std_125, test_rms_std_125, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                    test_target, train_mean, train_std * 1.25,\n",
    "                                                    phi_sigmoid)\n",
    "\n",
    "    train_rms_std_125_list.append(train_rms_std_125)\n",
    "    test_rms_std_125_list.append(test_rms_std_125)\n",
    "\n",
    "    # 150%\n",
    "    train_rms_std_150, test_rms_std_150, _ = fit_ml(train_feature, train_target, test_feature,\n",
    "                                                    test_target, train_mean, train_std * 1.5,\n",
    "                                                    phi_sigmoid)\n",
    "\n",
    "    train_rms_std_150_list.append(train_rms_std_150)\n",
    "    test_rms_std_150_list.append(test_rms_std_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('N-fold maximum likelihood:')\n",
    "\n",
    "pd.DataFrame(\n",
    "    data={\n",
    "        'train rms': [\n",
    "            np.mean(train_rms_std_50_list),\n",
    "            np.mean(train_rms_std_75_list),\n",
    "            np.mean(train_rms_std_100_list),\n",
    "            np.mean(train_rms_std_125_list),\n",
    "            np.mean(train_rms_std_150_list),\n",
    "        ],\n",
    "        'test rms': [\n",
    "            np.mean(test_rms_std_50_list),\n",
    "            np.mean(test_rms_std_75_list),\n",
    "            np.mean(test_rms_std_100_list),\n",
    "            np.mean(test_rms_std_125_list),\n",
    "            np.mean(test_rms_std_150_list),\n",
    "        ]\n",
    "    },\n",
    "    index=[\n",
    "        '50% std',\n",
    "        '75% std',\n",
    "        '100% std',\n",
    "        '125% std',\n",
    "        '150% std',\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the best std is 100% of the original std."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum A Posteriori Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Approach\n",
    "\n",
    "Maximize the likelihood of the data given the model, where the likelihood $ p(t | \\textbf{x}, \\textbf{w}, \\beta) = \\mathcal{N}(t | y(\\textbf{x}, \\textbf{w}),\\beta^{-1}) $.\n",
    "\n",
    "This approach is equivalent to minimizing the mean square error $ \\textit{E}_{D}(\\textbf{w}) = \\frac{1}{2} \\sum^{N}_{n=1}{\\{t_{n} - \\textbf{w}^{T} \\phi(\\textbf{x}_{n}) \\}} $. The optimal solution is given by the normal equation $ \\textbf{w}_{ML} = (\\Phi^{T} \\Phi)^{-1} \\Phi^{T} \\textbf{t} $.\n",
    "\n",
    "### Maximum A Posteriori Approach\n",
    "\n",
    "Assume that the prior distribution of the weight is a Gaussian distribution with mean $ 0 $ and variance $ \\alpha^{-1} $, which is $ p(\\textbf{w} | \\alpha) = \\mathcal{N}(\\textbf{w} | \\textbf{0}, \\alpha^{-1}\\textbf{I}) $.\n",
    "\n",
    "Maximize the posterior probability of the model given the data, where the posterior probability $ p(\\textbf{w} | \\textbf{t}, \\textbf{x}, \\alpha, \\beta) = \\mathcal{N}(\\textbf{w} | \\beta(\\alpha\\textbf{I} + \\beta\\Phi^{T}\\Phi)^{-1}\\Phi^{T}\\textbf{t}, (\\alpha\\textbf{I} + \\beta\\Phi^{T}\\Phi)^{-1}) $.\n",
    "\n",
    "This approach is equivalent to minimizing the mean square error $ \\textit{E}_{D}(\\textbf{w}) = \\frac{1}{2} \\sum^{N}_{n=1}{\\{t_{n} - \\textbf{w}^{T} \\phi(\\textbf{x}_{n}) \\}} + \\frac{\\lambda}{2} \\textbf{w}^{T} \\textbf{w} $, where $ \\lambda = \\frac{\\alpha}{\\beta} $. The optimal solution is given by the normal equation $ \\textbf{w}_{MAP} = (\\lambda\\textbf{I} + \\Phi^{T} \\Phi)^{-1} \\Phi^{T} \\textbf{t} $.\n",
    "\n",
    "With assuming the distribution of the weight, the model is more robust to the noise of the data. The model with the maximum a posteriori approach is more stable than the model with the maximum likelihood approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_weight_map(phi, target, lamb):\n",
    "    return np.linalg.inv((np.eye(phi.shape[1]) * lamb + phi.T @ phi)) @ phi.T @ target\n",
    "\n",
    "\n",
    "def fit_map(train_feature, train_target, test_feature, test_target, mean, std, phi_fn, lamb):\n",
    "    \"\"\"Fits the phi function.\n",
    "    \n",
    "    Args:\n",
    "        train_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        train_target: np.ndarray with shape (n_samples, 1)\n",
    "        test_feature: np.ndarray with shape (n_samples, n_features)\n",
    "        test_target: np.ndarray with shape (n_samples, 1)\n",
    "        mean: np.ndarray with shape (1, n_features)\n",
    "        std: np.ndarray with shape (1, n_features)\n",
    "        phi_fn: function with signature (features, mean, std) -> phi\n",
    "        lamb: float\n",
    "        \n",
    "    Returns:\n",
    "        train_rms: float\n",
    "        test_rms: float\n",
    "        weights: np.ndarray with shape (phi.shape[1], 1)\n",
    "    \"\"\"\n",
    "    train_phi = phi_fn(train_feature, mean, std)\n",
    "    test_phi = phi_fn(test_feature, mean, std)\n",
    "    weights = optimal_weight_map(train_phi, train_target, lamb)\n",
    "    train_rms = root_mean_square_error(train_phi @ weights, train_target)\n",
    "    test_rms = root_mean_square_error(test_phi @ weights, test_target)\n",
    "    return train_rms, test_rms, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 0.1\n",
    "\n",
    "train_rms_m1_list, test_rms_m1_list = [], []\n",
    "train_rms_m2_list, test_rms_m2_list = [], []\n",
    "train_rms_gaussian_list, test_rms_gaussian_list = [], []\n",
    "train_rms_sigmoid_list, test_rms_sigmoid_list = [], []\n",
    "train_rms_hybrid_list, test_rms_hybrid_list = [], []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(n_fold(dataset, 5)):\n",
    "    train_feature, train_target = dataset[train_index, :-1], dataset[train_index, -1]\n",
    "    test_feature, test_target = dataset[test_index, :-1], dataset[test_index, -1]\n",
    "\n",
    "    train_mean, train_std = get_mean_and_std(train_feature)\n",
    "\n",
    "    # polynomial m=1\n",
    "    train_rms_m1, test_rms_m1, _ = fit_map(train_feature, train_target, test_feature, test_target,\n",
    "                                           train_mean, train_std, lambda x, _m, _s: phi_m1(x),\n",
    "                                           LAMBDA)\n",
    "\n",
    "    train_rms_m1_list.append(train_rms_m1)\n",
    "    test_rms_m1_list.append(test_rms_m1)\n",
    "\n",
    "    # polynomial m=2\n",
    "    train_rms_m2, test_rms_m2, _ = fit_map(train_feature, train_target, test_feature, test_target,\n",
    "                                           train_mean, train_std, lambda x, _m, _s: phi_m2(x),\n",
    "                                           LAMBDA)\n",
    "\n",
    "    train_rms_m2_list.append(train_rms_m2)\n",
    "    test_rms_m2_list.append(test_rms_m2)\n",
    "\n",
    "    # gaussian\n",
    "    train_rms_gaussian, test_rms_gaussian, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                       test_target, train_mean, train_std,\n",
    "                                                       phi_gaussian, LAMBDA)\n",
    "\n",
    "    train_rms_gaussian_list.append(train_rms_gaussian)\n",
    "    test_rms_gaussian_list.append(test_rms_gaussian)\n",
    "\n",
    "    # sigmoid\n",
    "    train_rms_sigmoid, test_rms_sigmoid, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                     test_target, train_mean, train_std,\n",
    "                                                     phi_sigmoid, LAMBDA)\n",
    "\n",
    "    train_rms_sigmoid_list.append(train_rms_sigmoid)\n",
    "    test_rms_sigmoid_list.append(test_rms_sigmoid)\n",
    "\n",
    "    # hybrid\n",
    "    train_rms_hybrid, test_rms_hybrid, _ = fit_map(train_feature, train_target, test_feature,\n",
    "                                                   test_target, train_mean, train_std, phi_hybrid,\n",
    "                                                   LAMBDA)\n",
    "\n",
    "    train_rms_hybrid_list.append(train_rms_hybrid)\n",
    "    test_rms_hybrid_list.append(test_rms_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('N-fold maximum a posteriori:')\n",
    "\n",
    "pd.DataFrame(\n",
    "    data={\n",
    "        'train rms': [\n",
    "            np.mean(train_rms_m1_list),\n",
    "            np.mean(train_rms_m2_list),\n",
    "            np.mean(train_rms_gaussian_list),\n",
    "            np.mean(train_rms_sigmoid_list),\n",
    "            np.mean(train_rms_hybrid_list),\n",
    "        ],\n",
    "        'test rms': [\n",
    "            np.mean(test_rms_m1_list),\n",
    "            np.mean(test_rms_m2_list),\n",
    "            np.mean(test_rms_gaussian_list),\n",
    "            np.mean(test_rms_sigmoid_list),\n",
    "            np.mean(test_rms_hybrid_list),\n",
    "        ]\n",
    "    },\n",
    "    index=[\n",
    "        'polynomial m=1',\n",
    "        'polynomial m=2',\n",
    "        'gaussian',\n",
    "        'sigmoid',\n",
    "        'hybrid',\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('hw1-UzpuqfK3-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "48c54e9d887ba70e25130763dcb376f8c7abe5f3594e79f8e17bee41a5eeb409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
